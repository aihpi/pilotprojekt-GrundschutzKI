{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Baseline (XML)\n",
        "\n",
        "Dieses Notebook zeigt die vollständige Baseline-Pipeline für RAG auf dem\n",
        "XML-Kompendium: Laden/Chunking, Embeddings, Qdrant-Ingestion, Retrieval-\n",
        "Smoke-Test, einfacher RAG-Chatbot und Evaluation mit RAGAS.\n",
        "\n",
        "**Hinweise:**\n",
        "- Passen Sie `CHUNK_SIZE`, `CHUNK_OVERLAP` und `top_k` je nach Qualität an.\n",
        "- Die Evaluation nutzt RAGAS-Metriken inkl. Noise Sensitivity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Voraussetzungen\n",
        "- `.env` in `notebooks/` anlegen (Kopie von `.env.example`)\n",
        "- Qdrant lokal mit Docker starten (Standard: `http://localhost:6333`)\n",
        "\n",
        "```bash\n",
        "docker pull qdrant/qdrant\n",
        "docker run -d --name qdrant \\\n",
        "  -p 6333:6333 -p 6334:6334 \\\n",
        "  -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
        "  qdrant/qdrant\n",
        "```\n",
        "\n",
        "- `uv sync` oder `pip install ...` mit `litellm`, `qdrant-client`, `python-dotenv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "21a1df8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Make notebooks/ importable when running from Jupyter\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "if (NOTEBOOK_DIR / \"litellm_client.py\").exists():\n",
        "    sys.path.insert(0, str(NOTEBOOK_DIR))\n",
        "elif (NOTEBOOK_DIR / \"notebooks\" / \"litellm_client.py\").exists():\n",
        "    sys.path.insert(0, str(NOTEBOOK_DIR / \"notebooks\"))\n",
        "\n",
        "from litellm_client import (\n",
        "    chat_completion,\n",
        "    get_embeddings,\n",
        "    get_qdrant_client,\n",
        "    load_llm_config,\n",
        "    load_vectordb_config,\n",
        ")\n",
        "\n",
        "DATA_PATH = Path(\"../data/grundschutz.xml\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347e7098",
      "metadata": {},
      "source": [
        "## 1) Vektor-Datenbank initialisieren\n",
        "Wir legen (falls nötig) die Collection an und prüfen die Verbindung.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6befe2d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/felixboelter/Documents/GitHub/pilotprojekt-GrundschutzKI/notebooks/litellm_client.py:177: UserWarning: Api key is used with an insecure connection.\n",
            "  return QdrantClient(url=url, api_key=cfg.api_key)\n"
          ]
        }
      ],
      "source": [
        "# Qdrant-Verbindung prüfen\n",
        "from qdrant_client.http import models as qmodels\n",
        "\n",
        "llm_cfg = load_llm_config()\n",
        "vec_cfg = load_vectordb_config()\n",
        "client = get_qdrant_client(vec_cfg)\n",
        "\n",
        "collection_name = vec_cfg.collection or \"grundschutz_xml\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812004c2",
      "metadata": {},
      "source": [
        "## 2) XML laden und in Text-Chunks aufteilen\n",
        "Wir extrahieren Text aus der XML, säubern leicht und erstellen Chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "506dd6be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28086"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# XML laden (robust gegen BOM/Encoding/HTML-Header)\n",
        "from io import BytesIO\n",
        "from lxml import etree as LET\n",
        "\n",
        "raw = DATA_PATH.read_bytes()\n",
        "\n",
        "# Entferne UTF-8 BOM falls vorhanden\n",
        "if raw.startswith(b\"\\xef\\xbb\\xbf\"):\n",
        "    raw = raw[3:]\n",
        "\n",
        "# Heuristisch zu Text dekodieren\n",
        "try:\n",
        "    text = raw.decode(\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    text = raw.decode(\"latin-1\", errors=\"ignore\")\n",
        "\n",
        "# Falls ein nicht-XML-Header davor steht, skippe bis zum ersten '<'\n",
        "lt = text.find(\"<\")\n",
        "if lt > 0:\n",
        "    text = text[lt:]\n",
        "\n",
        "if not text.lstrip().startswith(\"<\"):\n",
        "    preview = text[:200].replace(\"\\n\", \" \")\n",
        "    raise ValueError(f\"Datei sieht nicht wie XML aus. Vorschau: {preview}\")\n",
        "\n",
        "# LXML mit recover=True\n",
        "parser = LET.XMLParser(recover=True, encoding=\"utf-8\")\n",
        "try:\n",
        "    root = LET.fromstring(text.encode(\"utf-8\"), parser=parser)\n",
        "except LET.XMLSyntaxError as exc:\n",
        "    raise ValueError(\"XML konnte nicht geparst werden. Bitte Dateiformat pruefen.\") from exc\n",
        "\n",
        "# Heuristik: alle Textknoten sammeln\n",
        "texts = [t for t in root.itertext() if t and t.strip()]\n",
        "\n",
        "len(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0b6d8cfd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2077"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Einfache Chunking-Funktion (Zeichen-basiert)\n",
        "\n",
        "CHUNK_SIZE = 2000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "joined = \"\\n\".join(texts)\n",
        "\n",
        "chunks = []\n",
        "start = 0\n",
        "while start < len(joined):\n",
        "    end = start + CHUNK_SIZE\n",
        "    chunk = joined[start:end]\n",
        "    chunks.append(chunk)\n",
        "    start = end - CHUNK_OVERLAP\n",
        "\n",
        "len(chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b545769a",
      "metadata": {},
      "source": [
        "## 3) Embeddings erstellen und in Qdrant speichern\n",
        "Wir erzeugen Embeddings über LiteLLM und speichern in Qdrant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bc8287cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing embeddings 0 to 512 / 2077\n",
            "Processing embeddings 512 to 1024 / 2077\n",
            "Processing embeddings 1024 to 1536 / 2077\n",
            "Processing embeddings 1536 to 2048 / 2077\n",
            "Processing embeddings 2048 to 2077 / 2077\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2077, 4096)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Embeddings erzeugen\n",
        "# Hinweis: kann je nach Modell/Provider einige Zeit dauern.\n",
        "embeddings = get_embeddings(chunks, llm_cfg, batch_size=512)\n",
        "len(embeddings), len(embeddings[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b41b1bc7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xx/_1vpf06s35sd5ms16ydz8jhm0000gn/T/ipykernel_85299/1479253992.py:4: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  client.recreate_collection(\n"
          ]
        }
      ],
      "source": [
        "# Qdrant Collection anlegen und upsert\n",
        "vector_size = len(embeddings[0])\n",
        "\n",
        "client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=qmodels.VectorParams(size=vector_size, distance=qmodels.Distance.COSINE),\n",
        ")\n",
        "\n",
        "points = []\n",
        "for idx, (chunk, vector) in enumerate(zip(chunks, embeddings)):\n",
        "    points.append(\n",
        "        qmodels.PointStruct(\n",
        "            id=idx,\n",
        "            vector=vector,\n",
        "            payload={\"text\": chunk},\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Batch-Upsert, um Payload-Limits zu vermeiden\n",
        "BATCH_SIZE = 128\n",
        "for start in range(0, len(points), BATCH_SIZE):\n",
        "    batch = points[start : start + BATCH_SIZE]\n",
        "    client.upsert(collection_name=collection_name, points=batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "049ab58f",
      "metadata": {},
      "source": [
        "## 4) Schneller Test-Query\n",
        "Kleine Retrieval-Abfrage als Smoke-Test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fa476f8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing embeddings 0 to 1 / 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['t werden, ob die Organisationsstruktur für Informationssicherheit noch angemessen ist oder ob sie an neue Rahmenbedingungen angepasst werden muss.\\n\\n...\\n\\n\\n...\\n\\nISMS.1.A7 Festlegung von Sicherheitsmaßna',\n",
              " 'zur Absicherung sinnvoll umzusetzen ist.\\nSicherheitskonzept\\n\\n...\\n\\nEin Sicherheitskonzept dient zur Umsetzung der Sicherheitsstrategie und beschreibt die geplante Vorgehensweise, um die gesetzten Siche',\n",
              " 'f individuelle Informationsverbünde eingehen können, werden zur Darstellung der Gefährdungslage typische Szenarien zugrunde gelegt. Die folgenden spezifischen Bedrohungen und Schwachstellen sind für d',\n",
              " 'wirksam ist. Bei Bedarf SOLLTE die Vorgehensweise angepasst werden.\\n\\n...\\n\\nDER.2.1.A8 Aufbau von Organisationsstrukturen zur Behandlung von Sicherheitsvorfällen (S)\\nFür den Umgang mit Sicherheitsvorfäl',\n",
              " 'ent wird die Planungs-, Lenkungs- und Kontrollaufgabe bezeichnet, die erforderlich ist, um einen durchdachten und wirksamen Prozess zur Herstellung von Informationssicherheit aufzubauen und kontinuier']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Retrieval Test\n",
        "query = \"Wie gehe ich mit Sicherheitsmaßnahmen in der Organisation um?\"\n",
        "query_emb = get_embeddings([query], llm_cfg)[0]\n",
        "\n",
        "response = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    query=query_emb,\n",
        "    limit=5,\n",
        ")\n",
        "results = response.points\n",
        "\n",
        "[res.payload.get(\"text\", \"\")[:200] for res in results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8000bf86",
      "metadata": {},
      "source": [
        "## 5) Leichter RAG-Chatbot\n",
        "Wir holen Top-K Chunks aus Qdrant und schicken sie zusammen mit der Frage an GPT OSS 120B.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0d4b4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leichter RAG-Chatbot (Context + LLM)\n",
        "question = \"Was sind empfohlene organisatorische Sicherheitsmaßnahmen?\"\n",
        "\n",
        "# Retrieval\n",
        "query_emb = get_embeddings([question], llm_cfg)[0]\n",
        "results = client.query_points(collection_name=collection_name, query=query_emb, limit=5).points\n",
        "context = \"\".join([res.payload.get(\"text\", \"\") for res in results])\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent. Antworte kurz und cite den Kontext.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Frage: {question}\\n\\nKontext:\\n{context}\"},\n",
        "]\n",
        "\n",
        "response = chat_completion(messages, llm_cfg)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78993742",
      "metadata": {},
      "source": [
        "## 7) Evaluation mit RAGAS (CSV)\n",
        "Wir nutzen die Datei `GrundschutzKI_Fragen-Antworten-Fundstellen.csv` als Test-Set.\n",
        "Die Evaluation erstellt RAG-Antworten und vergleicht sie mit den erwarteten Antworten.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "21ac8add",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGAS: Build evaluation records + answers (from CSV + Qdrant)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from datasets import Dataset\n",
        "\n",
        "def _retrieve_contexts(question: str, k: int, client, collection_name: str, llm_cfg) -> List[str]:\n",
        "    query_emb = get_embeddings([question], llm_cfg, batch_size=1)[0]\n",
        "    results = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_emb,\n",
        "        limit=k,\n",
        "    ).points\n",
        "    return [res.payload.get(\"text\", \"\") for res in results]\n",
        "\n",
        "def _build_messages(question: str, contexts: list[str], fewshot: list[dict]) -> list[dict]:\n",
        "    context_text = \"\\n\\n\".join(contexts)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Beantworte die Frage kurz, präzise und nutze ausschließlich den gelieferten Kontext! Antworte auf Deutsch. Die Antwort sollte maximal 2 Sätze lang sein.\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    for ex in fewshot:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Frage: {ex['question']}\\n\\nKontext:\\n<BEISPIEL-KONTEXT>\",\n",
        "            }\n",
        "        )\n",
        "        messages.append({\"role\": \"assistant\", \"content\": ex[\"answer\"]})\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Frage: {question}\\n\\nKontext:\\n{context_text}\",\n",
        "        }\n",
        "    )\n",
        "    return messages\n",
        "\n",
        "def _batch_generate_answers(messages_list: list[list[dict]], llm_cfg, batch_size: int, concurrency: int) -> list[str]:\n",
        "    def _extract_content(resp) -> str:\n",
        "        if isinstance(resp, dict):\n",
        "            return resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "        if hasattr(resp, \"choices\"):\n",
        "            return resp.choices[0].message.content\n",
        "        raise TypeError(f\"Unexpected response type: {type(resp)}\")\n",
        "    import litellm\n",
        "\n",
        "    answers: list[str] = []\n",
        "    try:\n",
        "        if hasattr(litellm, \"batch_completion\"):\n",
        "            for start in range(0, len(messages_list), batch_size):\n",
        "                batch = messages_list[start : start + batch_size]\n",
        "                resp = litellm.batch_completion(\n",
        "                    model=llm_cfg.model,\n",
        "                    messages=batch,\n",
        "                    api_key=llm_cfg.api_key,\n",
        "                    api_base=llm_cfg.api_base,\n",
        "                )\n",
        "                if isinstance(resp, list):\n",
        "                    batch_answers = [_extract_content(r) for r in resp]\n",
        "                else:\n",
        "                    batch_answers = [r[\"message\"][\"content\"] for r in resp.get(\"data\", [])]\n",
        "                answers.extend(batch_answers)\n",
        "            return answers\n",
        "        raise AttributeError\n",
        "    except Exception:\n",
        "        async def _aget_one(msgs):\n",
        "            return await litellm.acompletion(\n",
        "                model=llm_cfg.model,\n",
        "                messages=msgs,\n",
        "                api_key=llm_cfg.api_key,\n",
        "                api_base=llm_cfg.api_base,\n",
        "            )\n",
        "def build_eval_records(\n",
        "    csv_path: str = \"../data/data_evaluation/GSKI_Fragen-Antworten-Fundstellen.csv\",\n",
        "    sample_size: int = 10,\n",
        "    top_k: int = 5,\n",
        "    batch_size: int = 16,\n",
        "    concurrency: int = 8,\n",
        ") -> list[dict]:\n",
        "    \"\"\"Create RAGAS records with answers using LiteLLM + Qdrant.\"\"\"\n",
        "    llm_cfg = load_llm_config()\n",
        "    vec_cfg = load_vectordb_config()\n",
        "    qdrant_client = get_qdrant_client(vec_cfg)\n",
        "    collection_name = vec_cfg.collection or \"grundschutz_xml\"\n",
        "\n",
        "    df = pd.read_csv(Path(csv_path), sep=\";\", encoding=\"utf-8-sig\")\n",
        "\n",
        "    # Few-shot from first row\n",
        "    first_row = df.iloc[0]\n",
        "    fewshot = [{\"question\": first_row[\"Frage\"], \"answer\": first_row[\"Antwort\"]}]\n",
        "\n",
        "    records = []\n",
        "    questions = []\n",
        "    contexts_list = []\n",
        "\n",
        "    for _, row in df.iloc[1 : 1 + sample_size].iterrows():\n",
        "        question = row[\"Frage\"]\n",
        "        ground_truth_answer = row[\"Antwort\"]\n",
        "        ground_truth_context = row[\"Fundstellen im IT-Grundschutz-Kompendium 2023\"]\n",
        "        contexts = _retrieve_contexts(question, top_k, qdrant_client, collection_name, llm_cfg)\n",
        "\n",
        "        questions.append(question)\n",
        "        contexts_list.append(contexts)\n",
        "        records.append({\"question\": question, \"answer\": \"\", \"contexts\": contexts, \"ground_truth_answer\": ground_truth_answer, \"ground_truth_context\": ground_truth_context})\n",
        "\n",
        "    messages_list = [_build_messages(q, ctx, fewshot) for q, ctx in zip(questions, contexts_list)]\n",
        "    answers = _batch_generate_answers(messages_list, llm_cfg, batch_size, concurrency)\n",
        "\n",
        "    for rec, ans in zip(records, answers):\n",
        "        rec[\"answer\"] = ans\n",
        "\n",
        "    return Dataset.from_list(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d6eec6ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n",
            "Processing embeddings 0 to 1 / 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'contexts', 'ground_truth_answer', 'ground_truth_context'],\n",
              "    num_rows: 41\n",
              "})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = build_eval_records(sample_size=50, top_k=3, batch_size=16, concurrency=10)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "47567f85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: Welche grundlegenden Sicherheitsmaßnahmen müssen beim Einrichten eines Webservers ergriffen werden?\n",
            "\n",
            "ANSWER: Beim Einrichten muss der IT‑Betrieb eine sichere Grundkonfiguration durchführen: Der Webserver‑Prozess ist mit einem Konto minimaler Rechte in einer gekapselten Umgebung bzw. auf einem eigenen (virtuellen) Server zu betreiben, sämtliche nicht benötigte Schreibrechte, Module und Funktionen zu deaktivieren und die Web‑Dateien so zu schützen, dass unbefugtes Lesen oder Ändern ausgeschlossen ist. Zusätzlich sind alle Verbindungen per HTTPS zu verschlüsseln und die Anbindung durch Firewalls bzw. optional durch eine Web‑Application‑Firewall abzusichern.\n",
            "\n",
            "GROUND TRUTH: Nach der Installation eines Webservers muss eine sichere Grundkonfiguration vorgenommen werden. Dazu gehört die Zuweisung des Webserver-Prozesses einem Konto mit minimalen Rechten, die Ausführung in einer gekapselten Umgebung (sofern unterstützt), sowie die Deaktivierung nicht benötigter Module und Funktionen. Ist eine gekapselte Umgebung nicht möglich, sollte jeder Webserver auf einem eigenen physischen oder virtuellen Server laufen. Dem Webserver-Dienst MÜSSEN alle nicht notwendige Schreibberechtigungen entzogen werden. Nicht benötigte Module und Funktionen des Webservers MÜSSEN deaktiviert werden.\n",
            "\n",
            "GROUND TRUTH CONTEXT: APP.3.2.A1 Sichere Konfiguration eines Webservers (B)\n",
            "Nachdem der IT-Betrieb einen Webserver installiert hat, MUSS er eine sichere Grundkonfiguration vornehmen. Dazu MUSS er insbesondere den Webserver-Prozess einem Konto mit minimalen Rechten zuweisen. Der Webserver MUSS in einer gekapselten Umgebung ausgeführt werden, sofern dies vom Betriebssystem unterstützt wird. Ist dies nicht möglich, SOLLTE jeder Webserver auf einem eigenen physischen oder virtuellen Server ausgeführt werden. Dem Webserver-Dienst MÜSSEN alle nicht notwendige Schreibberechtigungen entzogen werden.\n",
            "Nicht benötigte Module und Funktionen des Webservers MÜSSEN deaktiviert werden.\n"
          ]
        }
      ],
      "source": [
        "print(\"QUESTION:\", dataset[0]['question'])\n",
        "print()\n",
        "print(\"ANSWER:\", dataset[0]['answer'])\n",
        "print()\n",
        "print(\"GROUND TRUTH:\", dataset[0]['ground_truth_answer'])\n",
        "print()\n",
        "print(\"GROUND TRUTH CONTEXT:\", dataset[0]['ground_truth_context'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc65bde",
      "metadata": {},
      "source": [
        "## 8) Interpretation der RAGAS-Metriken\n",
        "**answer_relevancy**: Wie gut die Antwort die Frage adressiert (höher = besser).\n",
        "**faithfulness**: Wie gut die Antwort durch den gegebenen Kontext gedeckt ist (höher = weniger Halluzination).\n",
        "**context_precision**: Wie viel des gelieferten Kontextes wirklich relevant ist (höher = weniger Rauschen).\n",
        "**context_recall**: Wie viel der relevanten Informationen im Kontext enthalten ist (höher = besseres Retrieval).\n",
        "\n",
        "**Daumenregel:**\n",
        "- Hoher Recall + niedrige Relevanz ⇒ zu viel Kontext oder schlechte Antwortformulierung.\n",
        "- Hohe Precision + niedrige Faithfulness ⇒ Antwort nutzt Kontext nicht sauber.\n",
        "- Niedrige Precision + hohe Recall ⇒ Retrieval liefert viel, aber unpräzise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b533e491",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMConfig(api_base='http://10.127.129.0:4000/v1/', api_key='sk-kW0pG01NT9iGe4OhNlYFyw', model='openai/gpt-oss-120b', embedding_model='openai/octen-embedding-8b')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fcad9790",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGAS metrics with LiteLLM proxy config from .env\n",
        "from ragas.llms import llm_factory\n",
        "from ragas.embeddings.litellm_provider import LiteLLMEmbeddings\n",
        "from ragas.metrics.collections import ContextPrecision, ContextRecall, Faithfulness, AnswerCorrectness\n",
        "import instructor\n",
        "import litellm\n",
        "from litellm_client import load_llm_config\n",
        "from ragas.embeddings.litellm_provider import LiteLLMEmbeddings\n",
        "\n",
        "\n",
        "llm_cfg = load_llm_config()\n",
        "litellm.api_base = llm_cfg.api_base\n",
        "litellm.api_key = llm_cfg.api_key\n",
        "\n",
        "# Uses the model + api_base + api_key from .env (LiteLLM proxy)\n",
        "# from openai import AsyncOpenAI\n",
        "# client = AsyncOpenAI(api_key=llm_cfg.api_key, base_url=llm_cfg.api_base)\n",
        "client = instructor.from_litellm(litellm.acompletion, mode=instructor.Mode.MD_JSON)\n",
        "llm = llm_factory(\n",
        "    llm_cfg.model,\n",
        "    client=client,\n",
        "    adapter=\"litellm\",\n",
        "    model_args={\"temperature\": 0.2},\n",
        ")\n",
        "embeddings = LiteLLMEmbeddings(\n",
        "    model=llm_cfg.embedding_model,\n",
        "    api_key=llm_cfg.api_key,\n",
        "    api_base=llm_cfg.api_base,\n",
        "    encoding_format=\"float\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed67328",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "from ragas.metrics.collections import (\n",
        "    ContextPrecision,\n",
        "    ContextRecall,\n",
        "    Faithfulness,\n",
        "    AnswerCorrectness,\n",
        "    NoiseSensitivity\n",
        ")\n",
        "import asyncio\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "\n",
        "scorers = {\n",
        "    \"context_precision\": ContextPrecision(llm=llm),\n",
        "    \"context_recall\": ContextRecall(llm=llm),\n",
        "    \"faithfulness\": Faithfulness(llm=llm),\n",
        "    \"answer_correctness\": AnswerCorrectness(llm=llm, embeddings=embeddings),\n",
        "    \"noise_sensitivity\": NoiseSensitivity(llm=llm),\n",
        "    \"noise_sensitivity_irrelevant\": NoiseSensitivity(llm=llm, mode=\"irrelevant\"),\n",
        "}\n",
        "\n",
        "async def _score_row(row, sem):\n",
        "    async with sem:\n",
        "        return {\n",
        "            \"context_precision\": (await scorers[\"context_precision\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                reference=row[\"ground_truth_context\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "            \"context_recall\": (await scorers[\"context_recall\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                reference=row[\"ground_truth_context\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "            \"faithfulness\": (await scorers[\"faithfulness\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                response=row[\"answer\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "            \"answer_correctness\": (await scorers[\"answer_correctness\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                response=row[\"answer\"],\n",
        "                reference=row[\"ground_truth_answer\"],\n",
        "            )).value,\n",
        "            \"noise_sensitivity\": (await scorers[\"noise_sensitivity\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                response=row[\"answer\"],\n",
        "                reference=row[\"ground_truth_answer\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "            \"noise_sensitivity_irrelevant\": (await scorers[\"noise_sensitivity_irrelevant\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                response=row[\"answer\"],\n",
        "                reference=row[\"ground_truth_answer\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "        }\n",
        "\n",
        "async def score_dataset_batched(ds, batch_size=10, concurrency=5):\n",
        "    sem = asyncio.Semaphore(concurrency)\n",
        "    rows = list(ds)\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(rows), batch_size):\n",
        "        batch = rows[i : i + batch_size]\n",
        "        tasks = [asyncio.create_task(_score_row(r, sem)) for r in batch]\n",
        "        batch_results = await tqdm_asyncio.gather(\n",
        "            *tasks,\n",
        "            desc=f\"Batch {i//batch_size + 1}\",\n",
        "            total=len(tasks)\n",
        "        )\n",
        "        results.extend(batch_results)\n",
        "\n",
        "    return results\n",
        "\n",
        "scores = await score_dataset_batched(dataset, batch_size=41, concurrency=41)\n",
        "\n",
        "# scores is a list of dicts, one per row\n",
        "stats = {\n",
        "    k: {\n",
        "        \"avg\": sum(s[k] for s in scores) / len(scores),\n",
        "        \"min\": min(s[k] for s in scores),\n",
        "        \"max\": max(s[k] for s in scores),\n",
        "    }\n",
        "    for k in scores[0].keys()\n",
        "}\n",
        "print(stats)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ba37d4e2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**RAG Evaluation Summary (Percentages)**\n",
              "\n",
              "- **Context precision**: 89.1%  \n",
              "  (min 45.0%, max 100.0%)\n",
              "- **Context recall**: 89.1%  \n",
              "  (min 0.0%, max 100.0%)\n",
              "- **Faithfulness**: 71.2%  \n",
              "  (min 0.0%, max 100.0%)\n",
              "- **Answer correctness**: 61.1%  \n",
              "  (min 17.0%, max 98.1%)\n",
              "- **Noise sensitivity**: 38.6%  \n",
              "  (min 0.0%, max 100.0%)\n",
              "- **Noise sensitivity (irrelevant)**: 10.3%  \n",
              "  (min 0.0%, max 100.0%)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\n",
        "f\"\"\"**RAG Evaluation Summary (Percentages)**\n",
        "\n",
        "- **Context precision**: {stats['context_precision']['avg']*100:.1f}%  \n",
        "  (min {stats['context_precision']['min']*100:.1f}%, max {stats['context_precision']['max']*100:.1f}%)\n",
        "- **Context recall**: {stats['context_recall']['avg']*100:.1f}%  \n",
        "  (min {stats['context_recall']['min']*100:.1f}%, max {stats['context_recall']['max']*100:.1f}%)\n",
        "- **Faithfulness**: {stats['faithfulness']['avg']*100:.1f}%  \n",
        "  (min {stats['faithfulness']['min']*100:.1f}%, max {stats['faithfulness']['max']*100:.1f}%)\n",
        "- **Answer correctness**: {stats['answer_correctness']['avg']*100:.1f}%  \n",
        "  (min {stats['answer_correctness']['min']*100:.1f}%, max {stats['answer_correctness']['max']*100:.1f}%)\n",
        "- **Noise sensitivity**: {stats['noise_sensitivity']['avg']*100:.1f}%  \n",
        "  (min {stats['noise_sensitivity']['min']*100:.1f}%, max {stats['noise_sensitivity']['max']*100:.1f}%)\n",
        "- **Noise sensitivity (irrelevant)**: {stats['noise_sensitivity_irrelevant']['avg']*100:.1f}%  \n",
        "  (min {stats['noise_sensitivity_irrelevant']['min']*100:.1f}%, max {stats['noise_sensitivity_irrelevant']['max']*100:.1f}%)\n",
        "\"\"\"\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "725d05f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Keine auffällige Daumenregel‑Kombination erkannt.\n"
          ]
        }
      ],
      "source": [
        "def daumenregel_hinweise(\n",
        "    precision: float,\n",
        "    recall: float,\n",
        "    faithfulness: float,\n",
        "    *,\n",
        "    high: float = 0.75,\n",
        "    low: float = 0.5,\n",
        "):\n",
        "    hints = []\n",
        "\n",
        "    if recall >= high and precision <= low:\n",
        "        hints.append(\"Hoher Recall + niedrige Relevanz ⇒ zu viel Kontext oder schlechte Antwortformulierung.\")\n",
        "\n",
        "    if precision >= high and faithfulness <= low:\n",
        "        hints.append(\"Hohe Precision + niedrige Faithfulness ⇒ Antwort nutzt Kontext nicht sauber.\")\n",
        "\n",
        "    if precision <= low and recall >= high:\n",
        "        hints.append(\"Niedrige Precision + hohe Recall ⇒ Retrieval liefert viel, aber unpräzise.\")\n",
        "\n",
        "    if not hints:\n",
        "        hints.append(\"Keine auffällige Daumenregel‑Kombination erkannt.\")\n",
        "\n",
        "    return hints\n",
        "\n",
        "avg = stats\n",
        "hints = daumenregel_hinweise(\n",
        "    precision=avg[\"context_precision\"][\"avg\"],\n",
        "    recall=avg[\"context_recall\"][\"avg\"],\n",
        "    faithfulness=avg[\"faithfulness\"][\"avg\"],\n",
        ")\n",
        "\n",
        "for h in hints:\n",
        "    print(\"-\", h)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f874eb75",
      "metadata": {},
      "source": [
        "# 8) DSPy\n",
        "DSPy ist ein Framework, das LLM‑Prompts und Programme systematisch optimiert, statt sie nur manuell zu schreiben.\n",
        "Wir nutzen DSPy, um Antworten konsistenter, faktengetreuer und besser an unsere Aufgaben anzupassen.\n",
        "Gerade bei RAG hilft DSPy, die Nutzung des Kontextes zu verbessern und die Qualität der Antworten messbar zu steigern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "4a9cbd00",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "# DSPy LLM (OpenAI-compatible via LiteLLM proxy)\n",
        "dspy_llm = dspy.LM(\n",
        "    model=llm_cfg.model,\n",
        "    api_base=llm_cfg.api_base,\n",
        "    api_key=llm_cfg.api_key,\n",
        "    temperature=0.2,  # as recommended for benchmarking\n",
        ")\n",
        "\n",
        "dspy.configure(lm=dspy_llm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4c6273",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class RAGAnswer(dspy.Signature):\n",
        "    \"\"\"Answer the question using the provided context only.\"\"\"\n",
        "    question: str = dspy.InputField()\n",
        "    context: str = dspy.InputField()\n",
        "    answer: str = dspy.OutputField(desc=\"Antworte auf Deutsch, kurz und präzise, maximal 2–3 Sätze.\")\n",
        "\n",
        "class RAGModule(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.predict = dspy.Predict(RAGAnswer)\n",
        "\n",
        "    def forward(self, question, context):\n",
        "        return self.predict(question=question, context=context)\n",
        "\n",
        "rag = RAGModule()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "332a2dd8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- SAMPLE 0 ---\n",
            "QUESTION: Welche grundlegenden Sicherheitsmaßnahmen müssen beim Einrichten eines Webservers ergriffen werden?\n",
            "ANSWER: Beim Einrichten eines Webservers sind eine sichere Grundkonfiguration (Konto mit minimalen Rechten, Kapselung/Isolation, Deaktivierung nicht benötigter Module), restriktive Dateiberechtigungen und ein geschütztes WWW‑Wurzelverzeichnis, die Durchsetzung von HTTPS mit HSTS sowie sicheren HTTP‑Headern (Content‑Type, X‑Content‑Type‑Options, Cache‑Control) und sicheren Cookie‑Attributen (Secure, SameSite, HttpOnly) erforderlich. Zusätzlich müssen umfassende Protokollierung, regelmäßige Penetrationstests/Revisionen, Malware‑Scans für Uploads und, bei erhöhtem Schutzbedarf, eine Web‑Application‑Firewall eingesetzt werden.\n",
            "GROUND TRUTH: Nach der Installation eines Webservers muss eine sichere Grundkonfiguration vorgenommen werden. Dazu gehört die Zuweisung des Webserver-Prozesses einem Konto mit minimalen Rechten, die Ausführung in einer gekapselten Umgebung (sofern unterstützt), sowie die Deaktivierung nicht benötigter Module und Funktionen. Ist eine gekapselte Umgebung nicht möglich, sollte jeder Webserver auf einem eigenen physischen oder virtuellen Server laufen. Dem Webserver-Dienst MÜSSEN alle nicht notwendige Schreibberechtigungen entzogen werden. Nicht benötigte Module und Funktionen des Webservers MÜSSEN deaktiviert werden.\n",
            "\n",
            "--- SAMPLE 1 ---\n",
            "QUESTION: Wie müssen Webserver-Dateien geschützt werden?\n",
            "ANSWER: Webserver‑Dateien, insbesondere Skripte und Konfigurationsdateien, müssen so gesichert werden, dass weder unbefugtes Lesen noch Änderungen möglich sind. Der Webserver ist so zu konfigurieren, dass nur das definierte WWW‑Wurzelverzeichnis zugänglich ist, Verzeichnislisten deaktiviert werden und vertrauliche Dateien nicht in öffentlichen Verzeichnissen liegen; regelmäßige Kontrollen sind erforderlich.\n",
            "GROUND TRUTH: Alle Dateien auf dem Webserver, insbesondere Skripte und Konfigurationsdateien, müssen so geschützt werden, dass sie nicht unbefugt gelesen oder verändert werden können. Der Webserver muss so konfiguriert sein, dass er nur Dateien aus dem definierten WWW-Wurzelverzeichnis ausliefert. Nicht benötigte Funktionen zur Verzeichnisaufzählung müssen deaktiviert werden, und vertrauliche Daten dürfen nicht in öffentlichen Verzeichnissen gespeichert werden. Regelmäßige Überprüfungen sind erforderlich.\n",
            "\n",
            "--- SAMPLE 2 ---\n",
            "QUESTION: Welche Sicherheitsanforderungen gelten für Datei-Uploads und -Downloads über einen Webserver?\n",
            "ANSWER: Datei‑Uploads und -Downloads müssen vor der Bereitstellung auf Schadprogramme geprüft werden; es ist eine maximale Upload‑Größe zu definieren und ausreichend Speicherplatz zu reservieren. Zudem sind die Dateien in gesicherten Verzeichnissen zu lagern und alle Zugriffs‑ sowie Fehlversuche zu protokollieren.\n",
            "GROUND TRUTH: Alle mithilfe des Webservers veröffentlichten Dateien müssen vorher auf Schadsoftware geprüft werden. Es muss eine Maximalgröße für Datei-Uploads festgelegt und ausreichend Speicherplatz reserviert sein.\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    row = dataset[i]\n",
        "    context = \"\\n\\n\".join(row[\"contexts\"])\n",
        "    pred = rag(question=row[\"question\"], context=context)\n",
        "    print(f\"\\n--- SAMPLE {i} ---\")\n",
        "    print(\"QUESTION:\", row[\"question\"])\n",
        "    print(\"ANSWER:\", pred.answer)\n",
        "    print(\"GROUND TRUTH:\", row[\"ground_truth\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa20e39c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg cosine: 0.9184638717993329\n",
            "min: 0.7410496258342404 max: 0.9836617026475417\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    a = np.asarray(a); b = np.asarray(b)\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "# dspy_answers: Liste der DSPy-Outputs in gleicher Reihenfolge wie dataset\n",
        "# dataset[\"answer\"]: bisherige Antworten\n",
        "\n",
        "async def cosine_similarity_dspy_vs_baseline(dspy_answers, baseline_answers, embeddings):\n",
        "    sims = []\n",
        "    for dspy_ans, base_ans in zip(dspy_answers, baseline_answers):\n",
        "        emb1 = await embeddings.aembed_text(dspy_ans)\n",
        "        emb2 = await embeddings.aembed_text(base_ans)\n",
        "        sims.append(cosine_sim(emb1, emb2))\n",
        "    return sims\n",
        "dspy_answers = []\n",
        "for i in range(len(dataset)):\n",
        "    row = dataset[i]\n",
        "    context = \"\\n\\n\".join(row[\"contexts\"])\n",
        "    pred = rag(question=row[\"question\"], context=context)\n",
        "    dspy_answers.append(pred.answer)\n",
        "\n",
        "sims = await cosine_similarity_dspy_vs_baseline(\n",
        "    dspy_answers,\n",
        "    dataset[\"answer\"],\n",
        "    embeddings,\n",
        ")\n",
        "\n",
        "print(\"avg cosine:\", sum(sims) / len(sims))\n",
        "print(\"min:\", min(sims), \"max:\", max(sims))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Findings\n",
        "\n",
        "- TBD: Add key observations after running the notebook.\n",
        "- TBD: Summarize metrics/results (e.g., faithfulness/answer correctness).\n",
        "- TBD: Note any dataset or retrieval quality issues discovered.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pilotprojekt-grundschutzki",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
