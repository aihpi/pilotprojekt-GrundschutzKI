{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Baseline (XML)\n",
        "\n",
        "Dieses Notebook baut eine einfache RAG-Pipeline auf Basis der XML-Datei `data/grundschutz.xml`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Voraussetzungen\n",
        "- `.env` in `notebooks/` anlegen (Kopie von `.env.example`)\n",
        "- Qdrant lokal mit Docker starten (Standard: `http://localhost:6333`)\n",
        "\n",
        "```bash\n",
        "docker pull qdrant/qdrant\n",
        "docker run -d --name qdrant \\\n",
        "  -p 6333:6333 -p 6334:6334 \\\n",
        "  -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
        "  qdrant/qdrant\n",
        "```\n",
        "\n",
        "- `uv sync` oder `pip install ...` mit `litellm`, `qdrant-client`, `python-dotenv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "21a1df8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Make notebooks/ importable when running from Jupyter\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "if (NOTEBOOK_DIR / \"litellm_client.py\").exists():\n",
        "    sys.path.insert(0, str(NOTEBOOK_DIR))\n",
        "elif (NOTEBOOK_DIR / \"notebooks\" / \"litellm_client.py\").exists():\n",
        "    sys.path.insert(0, str(NOTEBOOK_DIR / \"notebooks\"))\n",
        "\n",
        "from litellm_client import (\n",
        "    chat_completion,\n",
        "    get_embeddings,\n",
        "    get_qdrant_client,\n",
        "    load_llm_config,\n",
        "    load_vectordb_config,\n",
        ")\n",
        "\n",
        "DATA_PATH = Path(\"../data/grundschutz.xml\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347e7098",
      "metadata": {},
      "source": [
        "## 1) Vektor-Datenbank initialisieren\n",
        "Wir legen (falls nötig) die Collection an und prüfen die Verbindung.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "6befe2d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/felixboelter/Documents/GitHub/pilotprojekt-GrundschutzKI/notebooks/litellm_client.py:177: UserWarning: Api key is used with an insecure connection.\n",
            "  return QdrantClient(url=url, api_key=cfg.api_key)\n"
          ]
        }
      ],
      "source": [
        "# Qdrant-Verbindung prüfen\n",
        "from qdrant_client.http import models as qmodels\n",
        "\n",
        "llm_cfg = load_llm_config()\n",
        "vec_cfg = load_vectordb_config()\n",
        "client = get_qdrant_client(vec_cfg)\n",
        "\n",
        "collection_name = vec_cfg.collection or \"grundschutz_xml\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "812004c2",
      "metadata": {},
      "source": [
        "## 2) XML laden und in Text-Chunks aufteilen\n",
        "Wir extrahieren Text aus der XML, säubern leicht und erstellen Chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506dd6be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# XML laden (robust gegen BOM/Encoding/HTML-Header)\n",
        "from io import BytesIO\n",
        "from lxml import etree as LET\n",
        "\n",
        "raw = DATA_PATH.read_bytes()\n",
        "\n",
        "# Entferne UTF-8 BOM falls vorhanden\n",
        "if raw.startswith(b\"\\xef\\xbb\\xbf\"):\n",
        "    raw = raw[3:]\n",
        "\n",
        "# Heuristisch zu Text dekodieren\n",
        "try:\n",
        "    text = raw.decode(\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    text = raw.decode(\"latin-1\", errors=\"ignore\")\n",
        "\n",
        "# Falls ein nicht-XML-Header davor steht, skippe bis zum ersten '<'\n",
        "lt = text.find(\"<\")\n",
        "if lt > 0:\n",
        "    text = text[lt:]\n",
        "\n",
        "if not text.lstrip().startswith(\"<\"):\n",
        "    preview = text[:200].replace(\"\\n\", \" \")\n",
        "    raise ValueError(f\"Datei sieht nicht wie XML aus. Vorschau: {preview}\")\n",
        "\n",
        "# LXML mit recover=True\n",
        "parser = LET.XMLParser(recover=True, encoding=\"utf-8\")\n",
        "try:\n",
        "    root = LET.fromstring(text.encode(\"utf-8\"), parser=parser)\n",
        "except LET.XMLSyntaxError as exc:\n",
        "    raise ValueError(\"XML konnte nicht geparst werden. Bitte Dateiformat pruefen.\") from exc\n",
        "\n",
        "# Heuristik: alle Textknoten sammeln\n",
        "texts = [t for t in root.itertext() if t and t.strip()]\n",
        "\n",
        "len(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b6d8cfd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Einfache Chunking-Funktion (Zeichen-basiert)\n",
        "\n",
        "CHUNK_SIZE = 4000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "joined = \"\\n\".join(texts)\n",
        "\n",
        "chunks = []\n",
        "start = 0\n",
        "while start < len(joined):\n",
        "    end = start + CHUNK_SIZE\n",
        "    chunk = joined[start:end]\n",
        "    chunks.append(chunk)\n",
        "    start = end - CHUNK_OVERLAP\n",
        "\n",
        "len(chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b545769a",
      "metadata": {},
      "source": [
        "## 3) Embeddings erstellen und in Qdrant speichern\n",
        "Wir erzeugen Embeddings über LiteLLM und speichern in Qdrant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8287cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embeddings erzeugen\n",
        "# Hinweis: kann je nach Modell/Provider einige Zeit dauern.\n",
        "embeddings = get_embeddings(chunks, llm_cfg, batch_size=512)\n",
        "len(embeddings), len(embeddings[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41b1bc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qdrant Collection anlegen und upsert\n",
        "vector_size = len(embeddings[0])\n",
        "\n",
        "client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=qmodels.VectorParams(size=vector_size, distance=qmodels.Distance.COSINE),\n",
        ")\n",
        "\n",
        "points = []\n",
        "for idx, (chunk, vector) in enumerate(zip(chunks, embeddings)):\n",
        "    points.append(\n",
        "        qmodels.PointStruct(\n",
        "            id=idx,\n",
        "            vector=vector,\n",
        "            payload={\"text\": chunk},\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Batch-Upsert, um Payload-Limits zu vermeiden\n",
        "BATCH_SIZE = 128\n",
        "for start in range(0, len(points), BATCH_SIZE):\n",
        "    batch = points[start : start + BATCH_SIZE]\n",
        "    client.upsert(collection_name=collection_name, points=batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "049ab58f",
      "metadata": {},
      "source": [
        "## 4) Schneller Test-Query\n",
        "Kleine Retrieval-Abfrage als Smoke-Test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa476f8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieval Test\n",
        "query = \"Wie gehe ich mit Sicherheitsmaßnahmen in der Organisation um?\"\n",
        "query_emb = get_embeddings([query], llm_cfg)[0]\n",
        "\n",
        "response = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    query=query_emb,\n",
        "    limit=5,\n",
        ")\n",
        "results = response.points\n",
        "\n",
        "[res.payload.get(\"text\", \"\")[:200] for res in results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8000bf86",
      "metadata": {},
      "source": [
        "## 5) Leichter RAG-Chatbot\n",
        "Wir holen Top-K Chunks aus Qdrant und schicken sie zusammen mit der Frage an GPT OSS 120B.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0d4b4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leichter RAG-Chatbot (Context + LLM)\n",
        "question = \"Was sind empfohlene organisatorische Sicherheitsmaßnahmen?\"\n",
        "\n",
        "# Retrieval\n",
        "query_emb = get_embeddings([question], llm_cfg)[0]\n",
        "results = client.query_points(collection_name=collection_name, query=query_emb, limit=5).points\n",
        "context = \"\".join([res.payload.get(\"text\", \"\") for res in results])\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent. Antworte kurz und cite den Kontext.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Frage: {question}\\n\\nKontext:\\n{context}\"},\n",
        "]\n",
        "\n",
        "response = chat_completion(messages, llm_cfg)\n",
        "response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78993742",
      "metadata": {},
      "source": [
        "## 7) Evaluation mit RAGAS (CSV)\n",
        "Wir nutzen die Datei `GrundschutzKI_Fragen-Antworten-Fundstellen.csv` als Test-Set.\n",
        "Die Evaluation erstellt RAG-Antworten und vergleicht sie mit den erwarteten Antworten.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "21ac8add",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGAS: Build evaluation records + answers (from CSV + Qdrant)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from datasets import Dataset\n",
        "\n",
        "def _retrieve_contexts(question: str, k: int, client, collection_name: str, llm_cfg) -> List[str]:\n",
        "    query_emb = get_embeddings([question], llm_cfg, batch_size=1)[0]\n",
        "    results = client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_emb,\n",
        "        limit=k,\n",
        "    ).points\n",
        "    return [res.payload.get(\"text\", \"\") for res in results]\n",
        "\n",
        "def _build_messages(question: str, contexts: list[str], fewshot: list[dict]) -> list[dict]:\n",
        "    context_text = \"\\n\\n\".join(contexts)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Beantworte die Frage kurz, präzise und nutze ausschließlich den gelieferten Kontext! Antworte auf Deutsch. Die Antwort sollte maximal 2 Sätze lang sein.\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    for ex in fewshot:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Frage: {ex['question']}\\n\\nKontext:\\n<BEISPIEL-KONTEXT>\",\n",
        "            }\n",
        "        )\n",
        "        messages.append({\"role\": \"assistant\", \"content\": ex[\"answer\"]})\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Frage: {question}\\n\\nKontext:\\n{context_text}\",\n",
        "        }\n",
        "    )\n",
        "    return messages\n",
        "\n",
        "def _batch_generate_answers(messages_list: list[list[dict]], llm_cfg, batch_size: int, concurrency: int) -> list[str]:\n",
        "    def _extract_content(resp) -> str:\n",
        "        if isinstance(resp, dict):\n",
        "            return resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "        if hasattr(resp, \"choices\"):\n",
        "            return resp.choices[0].message.content\n",
        "        raise TypeError(f\"Unexpected response type: {type(resp)}\")\n",
        "    import litellm\n",
        "\n",
        "    answers: list[str] = []\n",
        "    try:\n",
        "        if hasattr(litellm, \"batch_completion\"):\n",
        "            for start in range(0, len(messages_list), batch_size):\n",
        "                batch = messages_list[start : start + batch_size]\n",
        "                resp = litellm.batch_completion(\n",
        "                    model=llm_cfg.model,\n",
        "                    messages=batch,\n",
        "                    api_key=llm_cfg.api_key,\n",
        "                    api_base=llm_cfg.api_base,\n",
        "                )\n",
        "                if isinstance(resp, list):\n",
        "                    batch_answers = [_extract_content(r) for r in resp]\n",
        "                else:\n",
        "                    batch_answers = [r[\"message\"][\"content\"] for r in resp.get(\"data\", [])]\n",
        "                answers.extend(batch_answers)\n",
        "            return answers\n",
        "        raise AttributeError\n",
        "    except Exception:\n",
        "        async def _aget_one(msgs):\n",
        "            return await litellm.acompletion(\n",
        "                model=llm_cfg.model,\n",
        "                messages=msgs,\n",
        "                api_key=llm_cfg.api_key,\n",
        "                api_base=llm_cfg.api_base,\n",
        "            )\n",
        "def build_eval_records(\n",
        "    csv_path: str = \"../data/data_evaluation/GSKI_Fragen-Antworten-Fundstellen.csv\",\n",
        "    sample_size: int = 10,\n",
        "    top_k: int = 5,\n",
        "    batch_size: int = 16,\n",
        "    concurrency: int = 8,\n",
        ") -> list[dict]:\n",
        "    \"\"\"Create RAGAS records with answers using LiteLLM + Qdrant.\"\"\"\n",
        "    llm_cfg = load_llm_config()\n",
        "    vec_cfg = load_vectordb_config()\n",
        "    qdrant_client = get_qdrant_client(vec_cfg)\n",
        "    collection_name = vec_cfg.collection or \"grundschutz_xml\"\n",
        "\n",
        "    df = pd.read_csv(Path(csv_path), sep=\";\", encoding=\"utf-8-sig\")\n",
        "\n",
        "    # Few-shot from first row\n",
        "    first_row = df.iloc[0]\n",
        "    fewshot = [{\"question\": first_row[\"Frage\"], \"answer\": first_row[\"Antwort\"]}]\n",
        "\n",
        "    records = []\n",
        "    questions = []\n",
        "    contexts_list = []\n",
        "\n",
        "    for _, row in df.iloc[1 : 1 + sample_size].iterrows():\n",
        "        question = row[\"Frage\"]\n",
        "        ground_truth_answer = row[\"Antwort\"]\n",
        "        ground_truth_context = row[\"Fundstellen im IT-Grundschutz-Kompendium 2023\"]\n",
        "        contexts = _retrieve_contexts(question, top_k, qdrant_client, collection_name, llm_cfg)\n",
        "\n",
        "        questions.append(question)\n",
        "        contexts_list.append(contexts)\n",
        "        records.append({\"question\": question, \"answer\": \"\", \"contexts\": contexts, \"ground_truth_answer\": ground_truth_answer, \"ground_truth_context\": ground_truth_context})\n",
        "\n",
        "    messages_list = [_build_messages(q, ctx, fewshot) for q, ctx in zip(questions, contexts_list)]\n",
        "    answers = _batch_generate_answers(messages_list, llm_cfg, batch_size, concurrency)\n",
        "\n",
        "    for rec, ans in zip(records, answers):\n",
        "        rec[\"answer\"] = ans\n",
        "\n",
        "    return Dataset.from_list(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "d6eec6ed",
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../GrundschutzKI_Fragen-Antworten-Fundstellen.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = \u001b[43mbuild_eval_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m dataset\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mbuild_eval_records\u001b[39m\u001b[34m(csv_path, sample_size, top_k, batch_size, concurrency)\u001b[39m\n\u001b[32m     87\u001b[39m qdrant_client = get_qdrant_client(vec_cfg)\n\u001b[32m     88\u001b[39m collection_name = vec_cfg.collection \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgrundschutz_xml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8-sig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Few-shot from first row\u001b[39;00m\n\u001b[32m     93\u001b[39m first_row = df.iloc[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/pilotprojekt-GrundschutzKI/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/pilotprojekt-GrundschutzKI/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/pilotprojekt-GrundschutzKI/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/pilotprojekt-GrundschutzKI/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/pilotprojekt-GrundschutzKI/.venv/lib/python3.12/site-packages/pandas/io/common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../GrundschutzKI_Fragen-Antworten-Fundstellen.csv'"
          ]
        }
      ],
      "source": [
        "dataset = build_eval_records(sample_size=50, top_k=5, batch_size=16, concurrency=10)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "47567f85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: Welche grundlegenden Sicherheitsmaßnahmen müssen beim Einrichten eines Webservers ergriffen werden?\n",
            "\n",
            "ANSWER: Beim Einrichten eines Webservers muss zunächst eine sichere Grundkonfiguration vorgenommen werden – der Webserver‑Prozess läuft unter einem Konto mit minimalen Rechten in einer gekapselten Umgebung, nicht benötigte Module und Schreibrechte werden deaktiviert, das WWW‑Wurzelverzeichnis wird strikt eingegrenzt und alle Dateien sowie Konfigurationen vor unbefugtem Zugriff geschützt. Zusätzlich sind restriktive HTTP‑Header (z. B. Strict‑Transport‑Security, Content‑Type, X‑Content‑Type‑Options, Cache‑Control), sichere Cookie‑Attribute (secure, SameSite, httponly), der Einsatz einer Web‑Application‑Firewall, umfassende Protokollierung, kryptografisch gesicherte Passwortspeicherung, Prüfung von Datei‑Uploads sowie regelmäßige Penetrationstests und Revisionen erforderlich.\n",
            "\n",
            "GROUND TRUTH: Nach der Installation eines Webservers muss eine sichere Grundkonfiguration vorgenommen werden. Dazu gehört die Zuweisung des Webserver-Prozesses einem Konto mit minimalen Rechten, die Ausführung in einer gekapselten Umgebung (sofern unterstützt), sowie die Deaktivierung nicht benötigter Module und Funktionen. Ist eine gekapselte Umgebung nicht möglich, sollte jeder Webserver auf einem eigenen physischen oder virtuellen Server laufen. Dem Webserver-Dienst MÜSSEN alle nicht notwendige Schreibberechtigungen entzogen werden. Nicht benötigte Module und Funktionen des Webservers MÜSSEN deaktiviert werden.\n"
          ]
        }
      ],
      "source": [
        "print(\"QUESTION:\", dataset[0]['question'])\n",
        "print()\n",
        "print(\"ANSWER:\", dataset[0]['answer'])\n",
        "print()\n",
        "print(\"GROUND TRUTH:\", dataset[0]['ground_truth'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cc65bde",
      "metadata": {},
      "source": [
        "## 8) Interpretation der RAGAS-Metriken\n",
        "**answer_relevancy**: Wie gut die Antwort die Frage adressiert (höher = besser).\n",
        "**faithfulness**: Wie gut die Antwort durch den gegebenen Kontext gedeckt ist (höher = weniger Halluzination).\n",
        "**context_precision**: Wie viel des gelieferten Kontextes wirklich relevant ist (höher = weniger Rauschen).\n",
        "**context_recall**: Wie viel der relevanten Informationen im Kontext enthalten ist (höher = besseres Retrieval).\n",
        "\n",
        "**Daumenregel:**\n",
        "- Hoher Recall + niedrige Relevanz ⇒ zu viel Kontext oder schlechte Antwortformulierung.\n",
        "- Hohe Precision + niedrige Faithfulness ⇒ Antwort nutzt Kontext nicht sauber.\n",
        "- Niedrige Precision + hohe Recall ⇒ Retrieval liefert viel, aber unpräzise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b533e491",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMConfig(api_base='http://10.127.129.0:4000/v1/', api_key='sk-kW0pG01NT9iGe4OhNlYFyw', model='openai/gpt-oss-120b', embedding_model='openai/octen-embedding-8b')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fcad9790",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAGAS metrics with LiteLLM proxy config from .env\n",
        "from ragas.llms import llm_factory\n",
        "from ragas.embeddings.litellm_provider import LiteLLMEmbeddings\n",
        "from ragas.metrics.collections import ContextPrecision, ContextRecall, Faithfulness, AnswerCorrectness\n",
        "import instructor\n",
        "import litellm\n",
        "from litellm_client import load_llm_config\n",
        "from ragas.embeddings.litellm_provider import LiteLLMEmbeddings\n",
        "\n",
        "\n",
        "llm_cfg = load_llm_config()\n",
        "litellm.api_base = llm_cfg.api_base\n",
        "litellm.api_key = llm_cfg.api_key\n",
        "\n",
        "# Uses the model + api_base + api_key from .env (LiteLLM proxy)\n",
        "# from openai import AsyncOpenAI\n",
        "# client = AsyncOpenAI(api_key=llm_cfg.api_key, base_url=llm_cfg.api_base)\n",
        "client = instructor.from_litellm(litellm.acompletion, mode=instructor.Mode.MD_JSON)\n",
        "llm = llm_factory(\n",
        "    llm_cfg.model,\n",
        "    client=client,\n",
        "    adapter=\"litellm\",\n",
        "    model_args={\"temperature\": 0.2},\n",
        ")\n",
        "embeddings = LiteLLMEmbeddings(\n",
        "    model=llm_cfg.embedding_model,\n",
        "    api_key=llm_cfg.api_key,\n",
        "    api_base=llm_cfg.api_base,\n",
        "    encoding_format=\"float\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "aed67328",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': {'avg': 0.9209010839804745, 'min': 0.249999999975, 'max': 0.99999999998}, 'context_recall': {'avg': 0.9735772357723577, 'min': 0.5, 'max': 1.0}, 'faithfulness': {'avg': 0.722765107521205, 'min': 0.0, 'max': 1.0}, 'answer_correctness': {'avg': 0.6200686538614504, 'min': 0.13106508565833477, 'max': 0.9853846488050791}}\n"
          ]
        }
      ],
      "source": [
        "from ragas.metrics.collections import (\n",
        "    ContextPrecision,\n",
        "    ContextRecall,\n",
        "    Faithfulness,\n",
        "    AnswerCorrectness,\n",
        ")\n",
        "import asyncio\n",
        "\n",
        "scorers = {\n",
        "    \"context_precision\": ContextPrecision(llm=llm),\n",
        "    \"context_recall\": ContextRecall(llm=llm),\n",
        "    \"faithfulness\": Faithfulness(llm=llm),\n",
        "    \"answer_correctness\": AnswerCorrectness(llm=llm, embeddings=embeddings),\n",
        "}\n",
        "\n",
        "async def _score_row(row, sem):\n",
        "    async with sem:\n",
        "        return {\n",
        "            \"context_precision\": (await scorers[\"context_precision\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                reference=row[\"ground_truth_context\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "            \"context_recall\": (await scorers[\"context_recall\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                reference=row[\"ground_truth_context\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "            \"faithfulness\": (await scorers[\"faithfulness\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                response=row[\"answer\"],\n",
        "                retrieved_contexts=row[\"contexts\"],\n",
        "            )).value,\n",
        "            \"answer_correctness\": (await scorers[\"answer_correctness\"].ascore(\n",
        "                user_input=row[\"question\"],\n",
        "                response=row[\"answer\"],\n",
        "                reference=row[\"ground_truth_answer\"],\n",
        "            )).value,\n",
        "        }\n",
        "\n",
        "async def score_dataset_batched(ds, batch_size=10, concurrency=5):\n",
        "    sem = asyncio.Semaphore(concurrency)\n",
        "    rows = list(ds)\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(rows), batch_size):\n",
        "        batch = rows[i : i + batch_size]\n",
        "        tasks = [asyncio.create_task(_score_row(r, sem)) for r in batch]\n",
        "        results.extend(await asyncio.gather(*tasks))\n",
        "\n",
        "    return results\n",
        "\n",
        "scores = await score_dataset_batched(dataset, batch_size=20, concurrency=20)\n",
        "\n",
        "# scores is a list of dicts, one per row\n",
        "stats = {\n",
        "    k: {\n",
        "        \"avg\": sum(s[k] for s in scores) / len(scores),\n",
        "        \"min\": min(s[k] for s in scores),\n",
        "        \"max\": max(s[k] for s in scores),\n",
        "    }\n",
        "    for k in scores[0].keys()\n",
        "}\n",
        "print(stats)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ba37d4e2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**RAG Evaluation Summary (Percentages)**\n",
              "\n",
              "- **Context precision**: 92.1%  \n",
              "  (min 25.0%, max 100.0%)\n",
              "- **Context recall**: 97.4%  \n",
              "  (min 50.0%, max 100.0%)\n",
              "- **Faithfulness**: 72.3%  \n",
              "  (min 0.0%, max 100.0%)\n",
              "- **Answer correctness**: 62.0%  \n",
              "  (min 13.1%, max 98.5%)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\n",
        "f\"\"\"**RAG Evaluation Summary (Percentages)**\n",
        "\n",
        "- **Context precision**: {stats['context_precision']['avg']*100:.1f}%  \n",
        "  (min {stats['context_precision']['min']*100:.1f}%, max {stats['context_precision']['max']*100:.1f}%)\n",
        "- **Context recall**: {stats['context_recall']['avg']*100:.1f}%  \n",
        "  (min {stats['context_recall']['min']*100:.1f}%, max {stats['context_recall']['max']*100:.1f}%)\n",
        "- **Faithfulness**: {stats['faithfulness']['avg']*100:.1f}%  \n",
        "  (min {stats['faithfulness']['min']*100:.1f}%, max {stats['faithfulness']['max']*100:.1f}%)\n",
        "- **Answer correctness**: {stats['answer_correctness']['avg']*100:.1f}%  \n",
        "  (min {stats['answer_correctness']['min']*100:.1f}%, max {stats['answer_correctness']['max']*100:.1f}%)\n",
        "\"\"\"\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "725d05f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Keine auffällige Daumenregel‑Kombination erkannt.\n"
          ]
        }
      ],
      "source": [
        "def daumenregel_hinweise(\n",
        "    precision: float,\n",
        "    recall: float,\n",
        "    faithfulness: float,\n",
        "    *,\n",
        "    high: float = 0.75,\n",
        "    low: float = 0.5,\n",
        "):\n",
        "    hints = []\n",
        "\n",
        "    if recall >= high and precision <= low:\n",
        "        hints.append(\"Hoher Recall + niedrige Relevanz ⇒ zu viel Kontext oder schlechte Antwortformulierung.\")\n",
        "\n",
        "    if precision >= high and faithfulness <= low:\n",
        "        hints.append(\"Hohe Precision + niedrige Faithfulness ⇒ Antwort nutzt Kontext nicht sauber.\")\n",
        "\n",
        "    if precision <= low and recall >= high:\n",
        "        hints.append(\"Niedrige Precision + hohe Recall ⇒ Retrieval liefert viel, aber unpräzise.\")\n",
        "\n",
        "    if not hints:\n",
        "        hints.append(\"Keine auffällige Daumenregel‑Kombination erkannt.\")\n",
        "\n",
        "    return hints\n",
        "\n",
        "avg = stats\n",
        "hints = daumenregel_hinweise(\n",
        "    precision=avg[\"context_precision\"][\"avg\"],\n",
        "    recall=avg[\"context_recall\"][\"avg\"],\n",
        "    faithfulness=avg[\"faithfulness\"][\"avg\"],\n",
        ")\n",
        "\n",
        "for h in hints:\n",
        "    print(\"-\", h)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f874eb75",
      "metadata": {},
      "source": [
        "# 8) DSPy\n",
        "DSPy ist ein Framework, das LLM‑Prompts und Programme systematisch optimiert, statt sie nur manuell zu schreiben.\n",
        "Wir nutzen DSPy, um Antworten konsistenter, faktengetreuer und besser an unsere Aufgaben anzupassen.\n",
        "Gerade bei RAG hilft DSPy, die Nutzung des Kontextes zu verbessern und die Qualität der Antworten messbar zu steigern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "4a9cbd00",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "# DSPy LLM (OpenAI-compatible via LiteLLM proxy)\n",
        "dspy_llm = dspy.LM(\n",
        "    model=llm_cfg.model,\n",
        "    api_base=llm_cfg.api_base,\n",
        "    api_key=llm_cfg.api_key,\n",
        "    temperature=0.2,  # as recommended for benchmarking\n",
        ")\n",
        "\n",
        "dspy.configure(lm=dspy_llm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4c6273",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class RAGAnswer(dspy.Signature):\n",
        "    \"\"\"Answer the question using the provided context only.\"\"\"\n",
        "    question: str = dspy.InputField()\n",
        "    context: str = dspy.InputField()\n",
        "    answer: str = dspy.OutputField(desc=\"Antworte auf Deutsch, kurz und präzise, maximal 2–3 Sätze.\")\n",
        "\n",
        "class RAGModule(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.predict = dspy.Predict(RAGAnswer)\n",
        "\n",
        "    def forward(self, question, context):\n",
        "        return self.predict(question=question, context=context)\n",
        "\n",
        "rag = RAGModule()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "332a2dd8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- SAMPLE 0 ---\n",
            "QUESTION: Welche grundlegenden Sicherheitsmaßnahmen müssen beim Einrichten eines Webservers ergriffen werden?\n",
            "ANSWER: Beim Einrichten eines Webservers sind eine sichere Grundkonfiguration (Konto mit minimalen Rechten, Kapselung/Isolation, Deaktivierung nicht benötigter Module), restriktive Dateiberechtigungen und ein geschütztes WWW‑Wurzelverzeichnis, die Durchsetzung von HTTPS mit HSTS sowie sicheren HTTP‑Headern (Content‑Type, X‑Content‑Type‑Options, Cache‑Control) und sicheren Cookie‑Attributen (Secure, SameSite, HttpOnly) erforderlich. Zusätzlich müssen umfassende Protokollierung, regelmäßige Penetrationstests/Revisionen, Malware‑Scans für Uploads und, bei erhöhtem Schutzbedarf, eine Web‑Application‑Firewall eingesetzt werden.\n",
            "GROUND TRUTH: Nach der Installation eines Webservers muss eine sichere Grundkonfiguration vorgenommen werden. Dazu gehört die Zuweisung des Webserver-Prozesses einem Konto mit minimalen Rechten, die Ausführung in einer gekapselten Umgebung (sofern unterstützt), sowie die Deaktivierung nicht benötigter Module und Funktionen. Ist eine gekapselte Umgebung nicht möglich, sollte jeder Webserver auf einem eigenen physischen oder virtuellen Server laufen. Dem Webserver-Dienst MÜSSEN alle nicht notwendige Schreibberechtigungen entzogen werden. Nicht benötigte Module und Funktionen des Webservers MÜSSEN deaktiviert werden.\n",
            "\n",
            "--- SAMPLE 1 ---\n",
            "QUESTION: Wie müssen Webserver-Dateien geschützt werden?\n",
            "ANSWER: Webserver‑Dateien, insbesondere Skripte und Konfigurationsdateien, müssen so gesichert werden, dass weder unbefugtes Lesen noch Änderungen möglich sind. Der Webserver ist so zu konfigurieren, dass nur das definierte WWW‑Wurzelverzeichnis zugänglich ist, Verzeichnislisten deaktiviert werden und vertrauliche Dateien nicht in öffentlichen Verzeichnissen liegen; regelmäßige Kontrollen sind erforderlich.\n",
            "GROUND TRUTH: Alle Dateien auf dem Webserver, insbesondere Skripte und Konfigurationsdateien, müssen so geschützt werden, dass sie nicht unbefugt gelesen oder verändert werden können. Der Webserver muss so konfiguriert sein, dass er nur Dateien aus dem definierten WWW-Wurzelverzeichnis ausliefert. Nicht benötigte Funktionen zur Verzeichnisaufzählung müssen deaktiviert werden, und vertrauliche Daten dürfen nicht in öffentlichen Verzeichnissen gespeichert werden. Regelmäßige Überprüfungen sind erforderlich.\n",
            "\n",
            "--- SAMPLE 2 ---\n",
            "QUESTION: Welche Sicherheitsanforderungen gelten für Datei-Uploads und -Downloads über einen Webserver?\n",
            "ANSWER: Datei‑Uploads und -Downloads müssen vor der Bereitstellung auf Schadprogramme geprüft werden; es ist eine maximale Upload‑Größe zu definieren und ausreichend Speicherplatz zu reservieren. Zudem sind die Dateien in gesicherten Verzeichnissen zu lagern und alle Zugriffs‑ sowie Fehlversuche zu protokollieren.\n",
            "GROUND TRUTH: Alle mithilfe des Webservers veröffentlichten Dateien müssen vorher auf Schadsoftware geprüft werden. Es muss eine Maximalgröße für Datei-Uploads festgelegt und ausreichend Speicherplatz reserviert sein.\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    row = dataset[i]\n",
        "    context = \"\\n\\n\".join(row[\"contexts\"])\n",
        "    pred = rag(question=row[\"question\"], context=context)\n",
        "    print(f\"\\n--- SAMPLE {i} ---\")\n",
        "    print(\"QUESTION:\", row[\"question\"])\n",
        "    print(\"ANSWER:\", pred.answer)\n",
        "    print(\"GROUND TRUTH:\", row[\"ground_truth\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa20e39c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg cosine: 0.9184638717993329\n",
            "min: 0.7410496258342404 max: 0.9836617026475417\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    a = np.asarray(a); b = np.asarray(b)\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "# dspy_answers: Liste der DSPy-Outputs in gleicher Reihenfolge wie dataset\n",
        "# dataset[\"answer\"]: bisherige Antworten\n",
        "\n",
        "async def cosine_similarity_dspy_vs_baseline(dspy_answers, baseline_answers, embeddings):\n",
        "    sims = []\n",
        "    for dspy_ans, base_ans in zip(dspy_answers, baseline_answers):\n",
        "        emb1 = await embeddings.aembed_text(dspy_ans)\n",
        "        emb2 = await embeddings.aembed_text(base_ans)\n",
        "        sims.append(cosine_sim(emb1, emb2))\n",
        "    return sims\n",
        "dspy_answers = []\n",
        "for i in range(len(dataset)):\n",
        "    row = dataset[i]\n",
        "    context = \"\\n\\n\".join(row[\"contexts\"])\n",
        "    pred = rag(question=row[\"question\"], context=context)\n",
        "    dspy_answers.append(pred.answer)\n",
        "\n",
        "sims = await cosine_similarity_dspy_vs_baseline(\n",
        "    dspy_answers,\n",
        "    dataset[\"answer\"],\n",
        "    embeddings,\n",
        ")\n",
        "\n",
        "print(\"avg cosine:\", sum(sims) / len(sims))\n",
        "print(\"min:\", min(sims), \"max:\", max(sims))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
