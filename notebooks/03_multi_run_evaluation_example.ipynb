{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cde30d",
   "metadata": {},
   "source": [
    "## Import the Multi-Run Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3fe174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == \"notebooks\" else NOTEBOOK_DIR\n",
    "\n",
    "if str(PROJECT_ROOT / \"scripts\") not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT / \"scripts\"))\n",
    "if str(PROJECT_ROOT / \"notebooks\") not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT / \"notebooks\"))\n",
    "\n",
    "from run_multi_evaluation import run_multi_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb430a2",
   "metadata": {},
   "source": [
    "## Function Parameters\n",
    "\n",
    "The `run_multi_evaluation()` function accepts the following parameters:\n",
    "\n",
    "### Required Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `llm` | str | LLM model identifier (e.g., `\"openai/gpt-oss-120b\"`) |\n",
    "| `embedding_model` | str | Embedding model identifier (e.g., `\"openai/octen-embedding-8b\"`) |\n",
    "| `input_data_description` | str | Free-text description of input data and preprocessing |\n",
    "| `chunk_size` | int | Character size of chunks in vector database |\n",
    "| `chunk_overlap` | int | Character overlap between chunks |\n",
    "| `top_k` | int | Number of chunks to retrieve per question |\n",
    "| `output_name` | str | Base name for output files (should include model info) |\n",
    "| `temperature` | float | LLM temperature setting |\n",
    "\n",
    "### Optional Parameters (with defaults)\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `num_runs` | int | 100 | Number of evaluation runs |\n",
    "| `master_seed` | int | 42 | Master seed for generating run seeds |\n",
    "| `eval_csv_path` | str | `\"data/data_evaluation/GSKI_Fragen-Antworten-Fundstellen.csv\"` | Path to evaluation CSV |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a352ed",
   "metadata": {},
   "source": [
    "## Example: Multi-Run Evaluation with GPT-OSS-120B\n",
    "\n",
    "**Note**: Running 100 evaluations will take a significant amount of time. For testing, you can reduce `num_runs` to a smaller value (e.g., 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7859d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-run evaluation (reduce num_runs for testing)\n",
    "results_path = run_multi_evaluation(\n",
    "    # Required parameters\n",
    "    llm=\"openai/gpt-oss-120b\",\n",
    "    embedding_model=\"openai/octen-embedding-8b\",\n",
    "    input_data_description=\"XML Kompendium 2023 (data/grundschutz.xml), character-based chunking with 4000 char chunks and 200 char overlap\",\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=200,\n",
    "    top_k=5,\n",
    "    output_name=\"gpt-oss-120b_kompendium-xml_multi\",  # Include model info in name\n",
    "    temperature=0.2,\n",
    "    \n",
    "    # Multi-run specific parameters\n",
    "    num_runs=5,  # Use 100 for full evaluation, 5 for testing\n",
    "    master_seed=42,\n",
    ")\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0adbcb",
   "metadata": {},
   "source": [
    "## Output Files\n",
    "\n",
    "The function generates three files in `data/results/`:\n",
    "\n",
    "1. **Per-question statistics** (`{output_name}_per_question.csv`):\n",
    "   - `Frage` - Question\n",
    "   - `{metric}_mean` - Mean value across runs\n",
    "   - `{metric}_min` - Minimum value across runs\n",
    "   - `{metric}_max` - Maximum value across runs\n",
    "   - `{metric}_std` - Standard deviation across runs\n",
    "\n",
    "2. **Overall statistics** (`{output_name}_overall.csv`):\n",
    "   - Aggregated statistics for all questions across all runs\n",
    "\n",
    "3. **README file** (`{output_name}.md`):\n",
    "   - Full documentation with configuration and summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652b14c",
   "metadata": {},
   "source": [
    "## View Generated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Read and display results\n",
    "results_dir = PROJECT_ROOT / \"data\" / \"results\"\n",
    "\n",
    "# List all multi-run result files\n",
    "print(\"Available multi-run result files:\")\n",
    "for f in sorted(results_dir.glob(\"*multi*\")):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-question statistics\n",
    "result_name = \"gpt-oss-120b_kompendium-xml_multi\"\n",
    "\n",
    "per_question_file = results_dir / f\"{result_name}_per_question.csv\"\n",
    "if per_question_file.exists():\n",
    "    per_question_df = pd.read_csv(per_question_file, sep=\";\", encoding=\"utf-8-sig\")\n",
    "    print(f\"Loaded {len(per_question_df)} questions\")\n",
    "    display(per_question_df.head())\n",
    "else:\n",
    "    print(f\"File not found: {per_question_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ef739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load overall statistics\n",
    "overall_file = results_dir / f\"{result_name}_overall.csv\"\n",
    "if overall_file.exists():\n",
    "    overall_df = pd.read_csv(overall_file, sep=\";\", encoding=\"utf-8-sig\")\n",
    "    print(\"Overall System Statistics:\")\n",
    "    display(overall_df)\n",
    "else:\n",
    "    print(f\"File not found: {overall_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0539cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the README\n",
    "readme_file = results_dir / f\"{result_name}.md\"\n",
    "if readme_file.exists():\n",
    "    display(Markdown(readme_file.read_text(encoding=\"utf-8\")))\n",
    "else:\n",
    "    print(f\"File not found: {readme_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de34637",
   "metadata": {},
   "source": [
    "## Analyze Variance\n",
    "\n",
    "Identify questions with high variance (may need more stable prompting or retrieval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3319f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'per_question_df' in dir() and per_question_df is not None:\n",
    "    # Find questions with highest variance in answer correctness\n",
    "    high_variance = per_question_df.nlargest(5, 'answer_correctness_std')[['Frage', 'answer_correctness_mean', 'answer_correctness_std']]\n",
    "    \n",
    "    display(Markdown(\"### Questions with Highest Answer Correctness Variance\"))\n",
    "    display(high_variance)\n",
    "    \n",
    "    # Find questions with lowest variance (most stable)\n",
    "    low_variance = per_question_df.nsmallest(5, 'answer_correctness_std')[['Frage', 'answer_correctness_mean', 'answer_correctness_std']]\n",
    "    \n",
    "    display(Markdown(\"### Questions with Lowest Answer Correctness Variance (Most Stable)\"))\n",
    "    display(low_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd368c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'per_question_df' in dir() and per_question_df is not None:\n",
    "    # Summary statistics of the variance itself\n",
    "    metrics = ['context_precision', 'context_recall', 'faithfulness', 'answer_correctness']\n",
    "    \n",
    "    variance_summary = []\n",
    "    for metric in metrics:\n",
    "        std_col = f\"{metric}_std\"\n",
    "        variance_summary.append({\n",
    "            'Metric': metric,\n",
    "            'Avg Std Dev': per_question_df[std_col].mean() * 100,\n",
    "            'Min Std Dev': per_question_df[std_col].min() * 100,\n",
    "            'Max Std Dev': per_question_df[std_col].max() * 100,\n",
    "        })\n",
    "    \n",
    "    variance_df = pd.DataFrame(variance_summary)\n",
    "    display(Markdown(\"### Variance Summary (Std Dev in Percentage Points)\"))\n",
    "    display(variance_df.round(2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
